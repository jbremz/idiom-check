{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IdiomCheck - Language Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(file_list):\n",
    "    '''\n",
    "    Inputs:\n",
    "    file_list - the list of paths of the text files to create the corpus\n",
    "    \n",
    "    Returns:\n",
    "    A list of strings containing the corpus\n",
    "    '''\n",
    "    corpus = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        with open(file_path) as f_input:\n",
    "            sample = re.sub('<[^>]+>', '', f_input.read()).replace('\\n', ' ').replace(r\"\\(.*\\)\",\"\").replace(\"(\",\"\").replace(\")\", \"\")[:10000]\n",
    "            if len(sample) > 0:\n",
    "                corpus.append(sample)\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(corpus, ngram_range=(1,3), rank_length=200):\n",
    "    '''\n",
    "    Inputs:\n",
    "    corpus - a list of strings\n",
    "    ngram_range - the range of ngram lengths to consider (defaults to 1-3)\n",
    "    rank_length - the length of the rank list to return (defaults to 200)\n",
    "    \n",
    "    Retuns:\n",
    "    rank - the array of ngrams ordered by frequency of occurrence in the corpus\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(input='content',ngram_range=(1,3), analyzer='char_wb')\n",
    "    transformed = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    sums = transformed.sum(axis=0)\n",
    "    \n",
    "    return np.array(features)[np.array(np.argsort(sums[0,:]))[0]][-rank_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sim(a, b):\n",
    "    '''\n",
    "    Takes two ranked arrays of the same shape and returns the average distance between the indices of matching elements\n",
    "    Elements which don't match are given a distance of the length of the array\n",
    "    '''\n",
    "    c = a[np.where(np.in1d(a,b))[0]]\n",
    "    d = c.reshape((len(c),1))\n",
    "    e = np.abs(np.where(np.in1d(a,b))[0]-np.where(d==b)[1])\n",
    "    no_match_penalty = (len(a)-len(e))*len(a) # TODO: make this condition more rigourous\n",
    "    \n",
    "    return (e.sum() + no_match_penalty)/len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the functions on some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovakian/Latvian similarity score: 127.535\n",
      "Slovakian/Slovakian similarity score: 46.435\n",
      "Latvian/Latvian. similarity score: 55.595\n"
     ]
    }
   ],
   "source": [
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])\n",
    "\n",
    "rank_sl_1 = rank(corpus_sl_1)\n",
    "rank_sl_2 = rank(corpus_sl_2)\n",
    "rank_lv_1 = rank(corpus_lv_1)\n",
    "rank_lv_2 = rank(corpus_lv_2)\n",
    "\n",
    "print('Slovakian/Latvian similarity score:', rank_sim(rank_sl_1, rank_lv_1))\n",
    "print('Slovakian/Slovakian similarity score:', rank_sim(rank_sl_1, rank_sl_2))\n",
    "print('Latvian/Latvian similarity score:', rank_sim(rank_lv_1, rank_lv_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both languages show similarity to themselves (a low score means a stronger agreement between their ranks), and less similarity between each other. In this simple binary classification, the two languages would have been told apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we will need to classify a given sample of text are the ranked ngrams for each language after analysing the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = next(os.walk('./txt'))[1]\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = {}\n",
    "\n",
    "for language in languages:\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    lang_corpus = corpus(file_list[:50])\n",
    "    lang_rank = rank(lang_corpus)\n",
    "    ranks[language] = lang_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "The test data comes as a .tsv file with the first column representing the language and the second column containing the string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('europarl-test.txt', sep='\\t', header=None, names=['language', 'string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>(CS) Най-голямата несправедливост на сегашната...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-жо председател, г-н член на Комисията, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, бих искал да започна с к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, въпросът за правата на ч...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1       bg  (CS) Най-голямата несправедливост на сегашната...\n",
       "2       bg  (DE) Г-жо председател, г-н член на Комисията, ...\n",
       "3       bg  (DE) Г-н председател, бих искал да започна с к...\n",
       "4       bg  (DE) Г-н председател, въпросът за правата на ч..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to strip the '(CS)','(DE)' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>Най-голямата несправедливост на сегашната общ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-жо председател, г-н член на Комисията, по п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-н председател, бих искал да започна с комен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-н председател, въпросът за правата на човек...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1       bg   Най-голямата несправедливост на сегашната общ...\n",
       "2       bg   Г-жо председател, г-н член на Комисията, по п...\n",
       "3       bg   Г-н председател, бих искал да започна с комен...\n",
       "4       bg   Г-н председател, въпросът за правата на човек..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['string'] = test['string'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to remove the rows with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace('', np.nan, inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20762, 2)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a test dataset of 20828 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu',\n",
       "       'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test.language.unique()).issubset(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore all of the languages in the test dataset are in the training dataset, this is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiom_check(lang_sample, ranks, print_scores=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "    lang_sample - the string to be classified\n",
    "    ranks - the dict() containing the language keys and their corresponding ranks\n",
    "    print_scores - bool - if True, the scores for each separate language is printed\n",
    "    \n",
    "    Returns:\n",
    "    The predicted language of lang_sample\n",
    "    '''\n",
    "    scores = {}\n",
    "    \n",
    "    sample_rank = rank([lang_sample])\n",
    "    \n",
    "    for key, rk in ranks.items():\n",
    "        scores[key] = rank_sim(sample_rank, rk)\n",
    "        \n",
    "    if print_scores:\n",
    "        print(scores)\n",
    "\n",
    "    return min(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test.copy()[:10000]\n",
    "results['pred_lang'] = results['string'].apply(lambda x: idiom_check(x, ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8344\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',(results['language'] == results['pred_lang']).sum()/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could also try a markov model at the character level and compare the likelihood for each sentence to be from a particular language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain MLE\n",
    "\n",
    "I aim to treat each string as a first-order Markov chain and then extract the character-level transition matrix for each language. Then I can calculate the $\\log(probability)$ for a particular string to belong to each language and choose the maximally likely option.\n",
    "\n",
    "There don't seem to be any Python packages for markov chains(?!) I will have to code this myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'A pangram, or holoalphabetic sentence, is a sentence that contains every letter of the alphabet at least once. The most famous pangram is probably the thirty-five-letter-long The quick brown fox jumps over the lazy dog which has been used to test typing equipment since at least the late'\n",
    "test2 = 'Jelutong is a suburb of George Town in Penang, Malaysia. Located south of the Pinang River, Jelutong has been inhabited since as early as the late 18th century, when traders from Aceh and India settled around the area.'\n",
    "test3 = 'Suzanne et Joseph étaient nés dans les deux premières années de leur arrivée à la colonie. Après la naissance de Suzanne, la mère abandonna l’enseignement d’état. Elle ne donna plus que des leçons particulières de français. Son mari avait été nommé directeur d’une école indigène et, disaient-elle, ils avaient vécu très largement malgré la charge de leurs enfants. Ces années-là furent sans conteste les meilleures de sa vie, des années de bonheur. Du moins c’étaient ce qu’elle disait. Elle s’en souvenait comme d’une terre lointaine et rêvée, d’une île. Elle en parlait de moins en moins à mesure qu’elle vieillissait, mais quand elle en parlait c’était toujours avec le même acharnement. Alors, à chaque fois, elle découvrait pour eux de nouvelles perfections à cette perfection, une nouvelle qualité à son mari, un nouvel aspect de l’aisance qu’ils connaissaient alors, et qui tendaient à devenir une opulence dont Joseph et Suzanne doutaient un peu.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From - stackoverflow.com/a/43413801\n",
    "\n",
    "def strided_axis0(a, L):\n",
    "    # Store the shape and strides info\n",
    "    shp = a.shape\n",
    "    s  = a.strides\n",
    "\n",
    "    # Compute length of output array along the first axis\n",
    "    nd0 = shp[0]-L+1\n",
    "\n",
    "    # Setup shape and strides for use with np.lib.stride_tricks.as_strided\n",
    "    # and get (n+1) dim output array\n",
    "    shp_in = (nd0,L)+shp[1:]\n",
    "    strd_in = (s[0],) + s\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shp_in, strides=strd_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the transitions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A', ' '],\n",
       "       [' ', 'p'],\n",
       "       ['p', 'a'],\n",
       "       ['a', 'n'],\n",
       "       ['n', 'g'],\n",
       "       ['g', 'r'],\n",
       "       ['r', 'a'],\n",
       "       ['a', 'm'],\n",
       "       ['m', ','],\n",
       "       [',', ' ']], dtype='<U1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = strided_axis0(test, 2)\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strided_axis0(test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(strided_axis0(test, 2), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are only 141 unique transitions out of 286 measured transitions - this is how we will build our transition matrix.\n",
    "\n",
    "Our transition matrix could end up being quite large. We already have over 16,000 entries for the 128 characters in US-ASCII, if we extend this to the Greek characters then this could be around 4,000,000 entries.\n",
    "\n",
    "It's probably best to store these as a (vectorised) numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_encode(groups, vocab=None):\n",
    "    '''\n",
    "    Input:\n",
    "    groups - array - groups of characters to be encoded\n",
    "    \n",
    "    Returns:\n",
    "    int_encoded - array - the integer encoded groups\n",
    "    vocab - array - an array where the index of each item corresponds to the integers in int_encoded: a lookup\n",
    "    \n",
    "    TODO: predefined vocab as input\n",
    "    '''\n",
    "    if type(vocab) == 'NoneType':\n",
    "        vocab = np.unique(groups)\n",
    "        \n",
    "    mask = (groups.flatten().reshape(groups.flatten().shape[0],1) == vocab)\n",
    "    onehot_encoded = np.zeros(mask.shape)\n",
    "    onehot_encoded[mask] = 1\n",
    "    int_encoded = np.argmax(onehot_encoded, axis=1).reshape(groups.shape)\n",
    "    \n",
    "    return int_encoded, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work out a way of dealing with the zero-valued probabilities - these are events that aren't need in the training data, but could be seen in the test data. For this it seems we should use some kind of [smoothing](https://pdfs.semanticscholar.org/5b2b/78087e51641a02966d6dcf20b51a5c43ccca.pdf). I should use _absolute discounting_ as it is easy to implement and apparently quite effective (would ideally use _Kneser-Ney smoothing_ but this would be very involved). [Here](http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf) is a good guide.\n",
    "\n",
    "However, this still seems too time-consuming. It may suffice to add a very small amount of probability to the zero values, we'll see when we test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(count_matrix):\n",
    "    '''\n",
    "    Smooths zero values with a small number << 1 (an altered form of Laplace smoothing)\n",
    "    \n",
    "    TODO: turn this into Kneser-Ney smoothing\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    count_matrix[count_matrix == 0] = 1e-10\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(text_list, vocab):\n",
    "    '''\n",
    "    Input:\n",
    "    text_list - list of strings - the texts to analyse\n",
    "    \n",
    "    Returns:\n",
    "    trans_mat - array - the Markovian transition matrix for the text given\n",
    "    vocab - array - an array where the index of each item corresponds to the position in trans_mat: a lookup\n",
    "    '''\n",
    "    count_mat_master = np.zeros((vocab.shape[0],vocab.shape[0])) # intialise the master count matrix with zero counts\n",
    "    \n",
    "    for text in tqdm(text_list):\n",
    "        text = np.array(list(text)) # prepare text as array of separate characters\n",
    "        pairs = strided_axis0(text, 2) # window characters into consecutive pairs\n",
    "        int_pairs, vocab = int_encode(pairs, vocab) # integer encode the characters\n",
    "        unique, counts = np.unique(int_pairs, return_counts=True, axis=0) # count the separate instances of the transitions\n",
    "        count_mat = np.zeros((vocab.shape[0],vocab.shape[0]))\n",
    "        count_mat[unique[:,0],unique[:,1]] = counts # populate the count matrix\n",
    "        count_mat_master += count_mat # add the counts to the master count matrix\n",
    "        \n",
    "    count_mat_master = smooth(count_mat_master)\n",
    "    trans_mat = count_mat_master/count_mat_master.sum(axis=1).reshape(count_mat_master.shape[0],1) # normalise the transition matrix (row stochastic)\n",
    "    \n",
    "    return trans_mat, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like(text, TM_list, vocab):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text - string - the sample to be analysed\n",
    "    TM_list - list of arrays - the list of transition matrices containing the transition probabilities for the calculation for each langauge\n",
    "    vocab - array - the characters corresponding to the transition matrix entries\n",
    "    \n",
    "    Returns:\n",
    "    log_likelihood - float - the log-likelihood for the string\n",
    "    \n",
    "    '''\n",
    "    TM_list = np.array(TM_list)\n",
    "    text = np.array(list(text))\n",
    "    pairs = strided_axis0(text, 2)\n",
    "    int_pairs, vocab = int_encode(pairs, vocab)\n",
    "    prob_lists = np.zeros((TM_list.shape[0], int_pairs.shape[0]))\n",
    "    \n",
    "    for i, TM in enumerate(TM_list):\n",
    "        prob_lists[i] = TM[int_pairs[:,0],int_pairs[:,1]]\n",
    "    \n",
    "    return np.log(prob_lists).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to extract my vocab from the training data, or I could create a sample of every character that's possible to create with two bytes of UTF-8? Let's try using the first $128 + 1920 = 2,048$ characters. This will be a very sparse matrix, but not too large for our means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([chr(i) for i in range(2048)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the routines on our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))\n",
    "\n",
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_list = [transition_matrix(corpus, vocab)[0] for corpus in [corpus_sl_1, corpus_lv_1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likes = np.zeros((len(corpus_sl_2),len(TM_list)))\n",
    "for i, sample in enumerate(corpus_sl_2):\n",
    "    log_likes[i,:] = log_like(sample, TM_list, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary detection accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "percentage_accuracy = 100*(np.where(log_likes[:,0] > log_likes[:,1])[0].shape[0] / log_likes.shape[0])\n",
    "print(\"Binary detection accuracy: %d%%\" % percentage_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model does indeed produce a larger log likelihood for the correct language, this is great. Now I need to create transition matrices for all of the languages and evaluate the model on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Markov Chain MLE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_list = []\n",
    "\n",
    "for language in tqdm(languages):\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    a_corpus = corpus(file_list[:50])\n",
    "    TM_list.append(transition_matrix(a_corpus, vocab)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(log_like(corpus_sl_1[28], TM_list, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sl'"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"nl\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nl'"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages[np.argmax(log_like(the_corpus[49], TM_list, vocab))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Change smoothing to post probabilities\n",
    "- Absolute Discounting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
