{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IdiomCheck - Language Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(file_list):\n",
    "    '''\n",
    "    Inputs:\n",
    "    file_list - the list of paths of the text files to create the corpus\n",
    "    \n",
    "    Returns:\n",
    "    A list of strings containing the corpus\n",
    "    '''\n",
    "    corpus = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        with open(file_path) as f_input:\n",
    "            sample = re.sub(r'\\([^)]*\\)', ' ', re.sub('<[^>]+>', '', f_input.read()).replace('\\n', ' '))[:20000]\n",
    "            sample = re.sub(r'/[^\\w\\s]/gi', '', sample)\n",
    "            sample = re.sub(r'[^\\u0000-\\u0800]', '', sample) # select only the first 2048 UTF-8 characters\n",
    "#             sample = sample.replace(chr(8221),\"\")\n",
    "            if len(sample) > 0:\n",
    "                corpus.append(sample)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(corpus, ngram_range=(1,3), rank_length=200):\n",
    "    '''\n",
    "    Inputs:\n",
    "    corpus - a list of strings\n",
    "    ngram_range - the range of ngram lengths to consider (defaults to 1-3)\n",
    "    rank_length - the length of the rank list to return (defaults to 200)\n",
    "    \n",
    "    Retuns:\n",
    "    rank - the array of ngrams ordered by frequency of occurrence in the corpus\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(input='content',ngram_range=(1,3), analyzer='char_wb')\n",
    "    transformed = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    sums = transformed.sum(axis=0)\n",
    "    \n",
    "    return np.array(features)[np.array(np.argsort(sums[0,:]))[0]][-rank_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sim(a, b):\n",
    "    '''\n",
    "    Takes two ranked arrays of the same shape and returns the average distance between the indices of matching elements\n",
    "    Elements which don't match are given a distance of the length of the array\n",
    "    '''\n",
    "    c = a[np.where(np.in1d(a,b))[0]]\n",
    "    d = c.reshape((len(c),1))\n",
    "    e = np.abs(np.where(np.in1d(a,b))[0]-np.where(d==b)[1])\n",
    "    no_match_penalty = (len(a)-len(e))*len(a) # TODO: make this condition more rigourous\n",
    "    \n",
    "    return (e.sum() + no_match_penalty)/len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Избор на квестори на Европейския парламент  : вж. протокола   . '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"bg\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])\n",
    "the_corpus[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have noticed a few issues with the samples, more cleaning needs to be done. This one above for example contains German (normally in brackets at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Избор на квестори на Европейския парламент  : вж. протокола   . '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"bg\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])\n",
    "the_corpus[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it has been removed, great. It has also removed all other text in brackets but this will be in a minority of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"cs\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Schválení zápisu z předchozího zasedání: viz zápis \n",
      " Členství ve výborech a delegacích: viz zápis \n",
      " Předložení dokumentů: viz zápis \n"
     ]
    }
   ],
   "source": [
    "print(the_corpus[0])\n",
    "print(the_corpus[1])\n",
    "print(the_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the early files contain the same ending which we'll need to strip as these will effect the frequency counts. Luckily each language has the same phrase after the colon in the same corresponding file. My guess is that this doesn't affect the overall statistics, given that it'll be lost amongst large text files. We can always come back and alter this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the functions on some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovakian/Latvian similarity score: 127.535\n",
      "Slovakian/Slovakian similarity score: 46.435\n",
      "Latvian/Latvian. similarity score: 55.595\n"
     ]
    }
   ],
   "source": [
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])\n",
    "\n",
    "rank_sl_1 = rank(corpus_sl_1)\n",
    "rank_sl_2 = rank(corpus_sl_2)\n",
    "rank_lv_1 = rank(corpus_lv_1)\n",
    "rank_lv_2 = rank(corpus_lv_2)\n",
    "\n",
    "print('Slovakian/Latvian similarity score:', rank_sim(rank_sl_1, rank_lv_1))\n",
    "print('Slovakian/Slovakian similarity score:', rank_sim(rank_sl_1, rank_sl_2))\n",
    "print('Latvian/Latvian similarity score:', rank_sim(rank_lv_1, rank_lv_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both languages show similarity to themselves (a low score means a stronger agreement between their ranks), and less similarity between each other. In this simple binary classification, the two languages would have been told apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we will need to classify a given sample of text are the ranked ngrams for each language after analysing the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = next(os.walk('./txt'))[1]\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a41eff3748c4c6dba80b921c76edf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ranks = {}\n",
    "\n",
    "for language in tqdm(languages):\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    lang_corpus = corpus(file_list[:500])\n",
    "    lang_rank = rank(lang_corpus)\n",
    "    ranks[language] = lang_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "The test data comes as a .tsv file with the first column representing the language and the second column containing the string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('europarl-test.txt', sep='\\t', header=None, names=['language', 'string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>(CS) Най-голямата несправедливост на сегашната...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-жо председател, г-н член на Комисията, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, бих искал да започна с к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, въпросът за правата на ч...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1       bg  (CS) Най-голямата несправедливост на сегашната...\n",
       "2       bg  (DE) Г-жо председател, г-н член на Комисията, ...\n",
       "3       bg  (DE) Г-н председател, бих искал да започна с к...\n",
       "4       bg  (DE) Г-н председател, въпросът за правата на ч..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to strip the '(CS)','(DE)' etc. As well as all the non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>Най-голямата несправедливост на сегашната общ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-жо председател, г-н член на Комисията, по п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-н председател, бих искал да започна с комен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-н председател, въпросът за правата на човек...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1       bg   Най-голямата несправедливост на сегашната общ...\n",
       "2       bg   Г-жо председател, г-н член на Комисията, по п...\n",
       "3       bg   Г-н председател, бих искал да започна с комен...\n",
       "4       bg   Г-н председател, въпросът за правата на човек..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['string'] = test['string'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "test['string'] = test['string'].str.replace(\"'\",\"\")\n",
    "test['string'] = test['string'].apply(lambda x: re.sub(r'/[^\\w\\s]/gi', '', x))\n",
    "test['string'] = test['string'].apply(lambda x: re.sub(r'[^\\u0000-\\u0800]', '', x))\n",
    "test = test[test['string'].apply(len) != 0] # remove empty strings\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to remove the rows with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20762, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a test dataset of 20828 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu',\n",
       "       'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test.language.unique()).issubset(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore all of the languages in the test dataset are in the training dataset, this is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiom_check(lang_sample, ranks, print_scores=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "    lang_sample - the string to be classified\n",
    "    ranks - the dict() containing the language keys and their corresponding ranks\n",
    "    print_scores - bool - if True, the scores for each separate language is printed\n",
    "    \n",
    "    Returns:\n",
    "    The predicted language of lang_sample\n",
    "    '''\n",
    "    scores = {}\n",
    "    \n",
    "    sample_rank = rank([lang_sample])\n",
    "    \n",
    "    for key, rk in ranks.items():\n",
    "        scores[key] = rank_sim(sample_rank, rk)\n",
    "        \n",
    "    if print_scores:\n",
    "        print(scores)\n",
    "\n",
    "    return min(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test.copy()[:10000]\n",
    "results['pred_lang'] = results['string'].apply(lambda x: idiom_check(x, ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.834\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',(results['language'] == results['pred_lang']).sum()/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't bad, but we could do better. Could also try a markov model at the character level and compare the likelihood for each sentence to be from a particular language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain MLE\n",
    "\n",
    "I aim to treat each string as a first-order Markov chain and then extract the character-level transition matrix for each language. Then I can calculate the $\\log(probability)$ for a particular string to belong to each language and choose the maximally likely option. Something along the lines of [this](https://pdfs.semanticscholar.org/2bf0/8addb83f51befa8b4bc7ed16b54ed34018d0.pdf) I suppose.\n",
    "\n",
    "There don't seem to be any Python packages for markov chains(?!) I will have to code this myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From - stackoverflow.com/a/43413801\n",
    "\n",
    "def strided_axis0(a, L):\n",
    "    # Store the shape and strides info\n",
    "    shp = a.shape\n",
    "    s  = a.strides\n",
    "\n",
    "    # Compute length of output array along the first axis\n",
    "    nd0 = shp[0]-L+1\n",
    "\n",
    "    # Setup shape and strides for use with np.lib.stride_tricks.as_strided\n",
    "    # and get (n+1) dim output array\n",
    "    shp_in = (nd0,L)+shp[1:]\n",
    "    strd_in = (s[0],) + s\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shp_in, strides=strd_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the transitions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'h'],\n",
       "       ['h', 'i'],\n",
       "       ['i', 's'],\n",
       "       ['s', ' '],\n",
       "       [' ', 'i'],\n",
       "       ['i', 's'],\n",
       "       ['s', ' '],\n",
       "       [' ', 'a'],\n",
       "       ['a', ' '],\n",
       "       [' ', 't']], dtype='<U1')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = \"This is a test\"\n",
    "test_arr = np.array(list(trial))\n",
    "pairs = strided_axis0(test_arr, 2)\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strided_axis0(test_arr, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(strided_axis0(test_arr, 2), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are only 11 unique transitions out of 13 measured transitions - this is how we will build our transition matrix.\n",
    "\n",
    "Our transition matrix could end up being quite large. We already have over 16,000 entries for the 128 characters in US-ASCII, if we extend this to the Greek characters then this could be around 4,000,000 entries.\n",
    "\n",
    "It's probably best to store these as a (vectorised) numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_encode(groups, encoder):\n",
    "    '''\n",
    "    Input:\n",
    "    groups - array - groups of characters to be encoded\n",
    "    encoder - a sklearn.preprocessing.LabelEncoder object which has been prefitted to a vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    int_encoded - array - the integer encoded groups\n",
    "    vocab - array - an array where the index of each item corresponds to the integers in int_encoded: a lookup\n",
    "    \n",
    "    TODO: predefined vocab as input\n",
    "    '''\n",
    "    flat_groups = groups.flatten()\n",
    "    try:\n",
    "        int_encoded = encoder.transform(flat_groups).reshape(groups.shape)\n",
    "    except ValueError:\n",
    "        print(''.join(list(groups[:,1])))\n",
    "    \n",
    "    return int_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work out a way of dealing with the zero-valued probabilities - these are events that aren't in the training data, but could be seen in the test data. For this it seems we should use some kind of [smoothing](https://pdfs.semanticscholar.org/5b2b/78087e51641a02966d6dcf20b51a5c43ccca.pdf). I should use _absolute discounting_ as it is easy to implement and apparently quite effective (would ideally use _Kneser-Ney smoothing_ but this would be very involved). [Here](http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf) is a good guide.\n",
    "\n",
    "However, this still seems too time-consuming. It may suffice to add a very small amount of probability to the zero values, we'll see when we test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(count_matrix):\n",
    "    '''\n",
    "    Smooths zero values with a small number << 1 (an altered form of Laplace smoothing)\n",
    "    \n",
    "    TODO: turn this into Kneser-Ney smoothing\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    count_matrix[count_matrix == 0] = 1e-10\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(trans_mat):\n",
    "    '''\n",
    "    Smooths zero values with a small number << 1 (an altered form of Laplace smoothing)\n",
    "    \n",
    "    TODO: turn this into Kneser-Ney smoothing\n",
    "    \n",
    "    '''\n",
    "    smooth_prob = trans_mat[(trans_mat != 0) & ~np.isnan(trans_mat)].min()/10 # take the minimum probability and choose something smaller than it\n",
    "    trans_mat[trans_mat == 0] = smooth_prob\n",
    "    trans_mat[np.isnan(trans_mat)] = smooth_prob\n",
    "    \n",
    "    return trans_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(text_list, vocab, encoder):\n",
    "    '''\n",
    "    Input:\n",
    "    text_list - list of strings - the texts to analyse\n",
    "    \n",
    "    Returns:\n",
    "    trans_mat - array - the Markovian transition matrix for the text given\n",
    "    vocab - array - an array where the index of each item corresponds to the position in trans_mat: a lookup\n",
    "    '''\n",
    "    count_mat_master = np.zeros((vocab.shape[0],vocab.shape[0])) # intialise the master count matrix with zero counts\n",
    "    \n",
    "    for text in tqdm(text_list):\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "        text = np.array(list(text)) # prepare text as array of separate characters\n",
    "        pairs = strided_axis0(text, 2) # window characters into consecutive pairs\n",
    "        int_pairs = int_encode(pairs, encoder) # integer encode the characters\n",
    "        unique, counts = np.unique(int_pairs, return_counts=True, axis=0) # count the separate instances of the transitions\n",
    "        count_mat = np.zeros((vocab.shape[0],vocab.shape[0]))\n",
    "        count_mat[unique[:,0],unique[:,1]] = counts # populate the count matrix\n",
    "        count_mat_master += count_mat # add the counts to the master count matrix\n",
    "        \n",
    "    count_mat_master = smooth(count_mat_master)\n",
    "    trans_mat = count_mat_master/count_mat_master.sum(axis=1).reshape(count_mat_master.shape[0],1) # normalise the transition matrix (row stochastic)\n",
    "    \n",
    "    return trans_mat, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like(text, TM_list, encoder):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text - string - the sample to be analysed\n",
    "    TM_list - array - the list of transition matrices containing the transition probabilities for the calculation for each langauge\n",
    "    vocab - array - the characters corresponding to the transition matrix entries\n",
    "    \n",
    "    Returns:\n",
    "    log_likelihood - float - the log-likelihood for the string\n",
    "    \n",
    "    '''\n",
    "    text = np.array(list(text))\n",
    "    pairs = strided_axis0(text, 2)\n",
    "    int_pairs = int_encode(pairs, encoder)\n",
    "    prob_lists = np.zeros((TM_list.shape[0], int_pairs.shape[0]))\n",
    "    \n",
    "    for i, TM in enumerate(TM_list):\n",
    "        prob_lists[i] = TM[int_pairs[:,0],int_pairs[:,1]]\n",
    "    \n",
    "    return np.log(prob_lists).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to extract my vocab from the training data, or I could create a sample of every character that's possible to create with two bytes of UTF-8? Let's try using the first $128 + 1920 = 2,048$ characters. We have filtered to these characters in our text preprocessing anyway. This will be a very sparse matrix, but not too large for our means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([chr(i) for i in range(2048)])\n",
    "encoder = LabelEncoder().fit(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Test\n",
    "\n",
    "Let's test the routines on our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))\n",
    "\n",
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de83c359b2d44a74a6aa104176f2d25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18f91e35a454cfbad168c0e4280c48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TM_list = np.array([transition_matrix(corpus, vocab, encoder)[0] for corpus in [corpus_sl_1, corpus_lv_1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likes = np.zeros((len(corpus_sl_2),len(TM_list)))\n",
    "for i, sample in enumerate(corpus_sl_2):\n",
    "    log_likes[i,:] = log_like(sample, TM_list, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary detection accuracy: 96%\n"
     ]
    }
   ],
   "source": [
    "percentage_accuracy = 100*(np.where(log_likes[:,0] > log_likes[:,1])[0].shape[0] / log_likes.shape[0])\n",
    "print(\"Binary detection accuracy: %d%%\" % percentage_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model does indeed produce a larger log likelihood for the correct language, this is great. Now I need to create transition matrices for all of the languages and evaluate the model on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Markov Chain MLE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e50937fbae4f618312e7dae93a3d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11d9b68057846ea87feea27aa4d0ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=498), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32954a4435b142129ba7eee79f06cd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c07400fe0d04edeb56bac3656ddbdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411757fa0dbd4dea84afa8c2a74c0272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3ec6a333f044c6949bfa06db69d9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccef99278754877af1f10ac554d38b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc97560c8ab45e5b9aaa3834fd44f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7e08c3807649388cfe9aa78adf60df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727c2f2b497b4dc19d55c131e5b41436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd8f02a47bd40679e5a0c3ccda5a2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f681c28b87644769406d85f14b9b865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fe61925f534f2eb547ac1990c1e9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619521a91b494f47a25075abe6cbab5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91274ad2b9874c8eb3da53b8caae5869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ea7cafe41745a1a785e755e0febaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22d623b87ff43fcb88e0653e1de69ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdb6c6ac58146e4a5af3a1e2b679117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1582eae425419c94b553f658ddff79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cb310407524e3889a4650bb95993f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f35ab45f3c4eefa2f7cee9d3deadc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396be25816464a998f93e18ed7ad7092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TM_list = np.zeros((len(languages), len(vocab), len(vocab)))\n",
    "\n",
    "for i, language in enumerate(tqdm(languages)):\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    a_corpus = corpus(file_list[:500])\n",
    "    TM_list[i] = transition_matrix(a_corpus, vocab, encoder)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the performance on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiom_check_mkv(lang_sample, TM_list, languages, encoder):\n",
    "    '''\n",
    "    Inputs:\n",
    "    lang_sample - the string to be classified\n",
    "    TM_list - array containing the transition matrices for each language\n",
    "    languages - list - list of the two character strings representing each language\n",
    "    encoder - sklearn.preprocessing.LabelEncoder obj - prefitted encoder\n",
    "    \n",
    "    Returns:\n",
    "    The predicted language of lang_sample - string\n",
    "    '''\n",
    "\n",
    "    return languages[np.argmax(log_like(lang_sample, TM_list, encoder))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test.copy()\n",
    "results['pred_lang'] = results['string'].apply(lambda x: idiom_check_mkv(x, TM_list, languages, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9707638955784607\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',(results['language'] == results['pred_lang']).sum()/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97% is quite good. I think at this point improvements would have to come either by using a larger sample for the transition matrices, or by improving on the smoothing technique (which is very rudimentary as it stands). Let's look to see where it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     607\n",
       "unique     18\n",
       "top        fi\n",
       "freq      345\n",
       "Name: pred_lang, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(results['language'] != results['pred_lang'])]['pred_lang'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>et</td>\n",
       "      <td>Sellise lähenemisega jäämegi ebakindlasse oluk...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>336</td>\n",
       "      <td>1</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       language                                             string pred_lang\n",
       "count       345                                                345       345\n",
       "unique        4                                                345         1\n",
       "top          et  Sellise lähenemisega jäämegi ebakindlasse oluk...        fi\n",
       "freq        336                                                  1       345"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(results['language'] != results['pred_lang']) & (results['pred_lang'] == 'fi')].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems almost all of the samples which were mistaken for Finnish were actually Estonian. These languages must be quite similar in terms of their character transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>et</td>\n",
       "      <td>Austatud juhataja! Tahaksin kõigepealt avalda...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>et</td>\n",
       "      <td>Euroopa Ülemkogu on väljendanud lootust, et v...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>et</td>\n",
       "      <td>Härra juhataja, me oleme rääkinud üleilmastum...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>et</td>\n",
       "      <td>Härra president, volinik, daamid ja härrad, h...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>et</td>\n",
       "      <td>Proua juhataja, Ria Oomen-Ruijten on esitanud...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                             string pred_lang\n",
       "6993       et   Austatud juhataja! Tahaksin kõigepealt avalda...        et\n",
       "6994       et   Euroopa Ülemkogu on väljendanud lootust, et v...        et\n",
       "6995       et   Härra juhataja, me oleme rääkinud üleilmastum...        et\n",
       "6996       et   Härra president, volinik, daamid ja härrad, h...        et\n",
       "6997       et   Proua juhataja, Ria Oomen-Ruijten on esitanud...        fi"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['language'] == 'et'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, haluaisin äänestää Hannes S...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, minulla on kaksi lisäkysymy...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, poikkean käsikirjoituksestani.</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, ennen kuin arvostelemme Ven...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, haluan kiittää näiden kahde...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                             string pred_lang\n",
       "7992       fi   Arvoisa puhemies, haluaisin äänestää Hannes S...        fi\n",
       "7993       fi   Arvoisa puhemies, minulla on kaksi lisäkysymy...        fi\n",
       "7994       fi   Arvoisa puhemies, poikkean käsikirjoituksestani.        fi\n",
       "7995       fi   Arvoisa puhemies, ennen kuin arvostelemme Ven...        fi\n",
       "7996       fi   Arvoisa puhemies, haluan kiittää näiden kahde...        fi"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['language'] == 'fi'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce the confusion matrix for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted   bg    cs    da   de   el    en    es   et    fi    fr   ...     \\\n",
      "Actual                                                              ...      \n",
      "bg         997     0     0    0    0     0     0    0     0     0   ...      \n",
      "cs           0   975     1    0    1     1     2    0     1     0   ...      \n",
      "da           0     0   991    0    0     0     0    0     0     0   ...      \n",
      "de           0     0     1  992    1     0     0    0     0     0   ...      \n",
      "el           0     0     0    0  988     0     0    0     0     0   ...      \n",
      "en           0     0     0    0    0   998     0    0     0     0   ...      \n",
      "es           0     0     0    0    0     0   994    0     0     0   ...      \n",
      "et           0     0     0    3    0     4     0  617   336     0   ...      \n",
      "fi           0     0     0    0    0     0     0    0   995     0   ...      \n",
      "fr           0     0     0    0    0     0     1    0     0   996   ...      \n",
      "hu           1     0     5    0    0     0     0    0     0     0   ...      \n",
      "it           0     0     0    1    0     0     0    0     0     0   ...      \n",
      "lt           0     0     3    1    0     1     1    0     5     1   ...      \n",
      "lv           0     0     0    0    0     1     0    0     0     0   ...      \n",
      "nl           0     0     0    0    0     0     0    0     0     0   ...      \n",
      "pl           0     0     0    1    1     1     0    0     0     0   ...      \n",
      "pt           0     0     0    0    0     0     0    0     0     0   ...      \n",
      "ro           0     0     0    0    0     1     4    0     0    14   ...      \n",
      "sk           0    43     1    0    0     0     6    0     0     0   ...      \n",
      "sl           0     1    13    1    0     0     6    0     3     0   ...      \n",
      "sv           0     0     4    0    0     0     0    0     0     0   ...      \n",
      "__all__    998  1019  1019  999  991  1007  1014  617  1340  1011   ...      \n",
      "\n",
      "Predicted   lt   lv    nl   pl    pt   ro   sk   sl    sv  __all__  \n",
      "Actual                                                              \n",
      "bg           0    0     0    0     0    0    0    0     0      997  \n",
      "cs           0    0     1    0     2    0    8    1     0      993  \n",
      "da           0    0     0    0     0    0    0    0     3      994  \n",
      "de           0    0     0    0     0    0    0    0     0      994  \n",
      "el           0    0     0    0     0    0    0    0     0      988  \n",
      "en           0    0     1    0     0    0    0    0     0      999  \n",
      "es           0    0     0    0     2    0    0    0     0      997  \n",
      "et           0    0    18    0     1    0    0    0    14      994  \n",
      "fi           0    0     0    0     0    0    0    0     0      995  \n",
      "fr           0    0     0    0     0    0    0    0     0      999  \n",
      "hu           0    0     2    0     2    0    0    0     1      998  \n",
      "it           0    0     0    0     0    0    0    0     0      996  \n",
      "lt         977    2     0    0     2    0    0    1     0      995  \n",
      "lv           1  973     1    0     2    0    0    0     0      978  \n",
      "nl           0    0   999    0     0    0    0    0     0      999  \n",
      "pl           0    0     0  996     0    0    0    0     0      999  \n",
      "pt           0    0     0    0   996    0    0    0     0      996  \n",
      "ro           0    0     0    0     5  894    0    0     0      928  \n",
      "sk           1    0     0    0     5    0  869    1     2      929  \n",
      "sl           0    0     2    2    10    0    3  934    13      998  \n",
      "sv           0    0     0    0     0    0    0    0   992      996  \n",
      "__all__    979  975  1024  998  1027  894  880  937  1025    20762  \n",
      "\n",
      "[22 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = ConfusionMatrix(results['language'], results['pred_lang'])\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are generally good apart from a very poor performance for Estonian. Interestingly, Estonian is mistaken for Finnish, but not vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estonian Accuracy: 0.6207243460764588\n"
     ]
    }
   ],
   "source": [
    "et_results = results[results['language'] == 'et']\n",
    "print('Estonian Accuracy:',((et_results['language'] == et_results['pred_lang'])).sum()/len(et_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Absolute Discounting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
