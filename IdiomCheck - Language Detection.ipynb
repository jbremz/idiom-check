{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IdiomCheck - Language Detection\n",
    "\n",
    "Here I detail my attempts to create a language classifier trained on the [European Parliament Proceedings Parallel Corpus](http://www.statmt.org/europarl/) and tested using [this](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/language-detection/europarl-test.zip) dataset.\n",
    "\n",
    "I use two main approaches:\n",
    "- N-gram frequency distribution comparison\n",
    "- Markov chain MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(file_list):\n",
    "    '''\n",
    "    Inputs:\n",
    "    file_list - the list of paths of the text files to create the corpus\n",
    "    \n",
    "    Returns:\n",
    "    A list of strings containing the corpus\n",
    "    '''\n",
    "    corpus = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        with open(file_path) as f_input:\n",
    "            sample = re.sub(r'\\([^)]*\\)', ' ', re.sub('<[^>]+>', '', f_input.read()).replace('\\n', ' '))[:20000] # only select the first 20000 characters for memory purposes\n",
    "            sample = re.sub(r'/[^\\w\\s]/gi', '', sample)\n",
    "            sample = re.sub(r'[^\\u0000-\\u0800]', '', sample) # select only the first 2048 UTF-8 characters\n",
    "            if len(sample) > 0:\n",
    "                corpus.append(sample)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Избор на квестори на Европейския парламент срок за депозиране на кандидатури: вж. протокола  Die Sitzung wird um 15.25 Uhr unterbrochen und um 18.00 Uhr wiederaufgenommen. '"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"bg\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])\n",
    "the_corpus[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Bulgarian sample above contains German (normally in brackets at the end), we need to remove this. Have altered `corpus()` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Избор на квестори на Европейския парламент  : вж. протокола   . '"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"bg\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])\n",
    "the_corpus[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it has been removed, great. It has also removed all other text in brackets but this will be in a minority of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"cs\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Schválení zápisu z předchozího zasedání: viz zápis \n",
      " Členství ve výborech a delegacích: viz zápis \n",
      " Předložení dokumentů: viz zápis \n"
     ]
    }
   ],
   "source": [
    "print(the_corpus[0])\n",
    "print(the_corpus[1])\n",
    "print(the_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the early files contain the same ending which we'll need to strip as these will effect the frequency counts. Luckily each language has the same phrase after the colon in the same corresponding file. My guess is that this doesn't affect the overall statistics, given that it'll be lost amongst large text files. We can always come back and alter this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "The test data comes as a .tsv file with the first column representing the language and the second column containing the string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('europarl-test.txt', sep='\\t', header=None, names=['language', 'string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>(CS) Най-голямата несправедливост на сегашната...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-жо председател, г-н член на Комисията, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, бих искал да започна с к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, въпросът за правата на ч...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1       bg  (CS) Най-голямата несправедливост на сегашната...\n",
       "2       bg  (DE) Г-жо председател, г-н член на Комисията, ...\n",
       "3       bg  (DE) Г-н председател, бих искал да започна с к...\n",
       "4       bg  (DE) Г-н председател, въпросът за правата на ч..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to strip the '(CS)','(DE)' etc. As well as all the non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>Най-голямата несправедливост на сегашната общ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-жо председател, г-н член на Комисията, по п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-н председател, бих искал да започна с комен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>Г-н председател, въпросът за правата на човек...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1       bg   Най-голямата несправедливост на сегашната общ...\n",
       "2       bg   Г-жо председател, г-н член на Комисията, по п...\n",
       "3       bg   Г-н председател, бих искал да започна с комен...\n",
       "4       bg   Г-н председател, въпросът за правата на човек..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['string'] = test['string'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "test['string'] = test['string'].str.replace(\"'\",\"\")\n",
    "test['string'] = test['string'].apply(lambda x: re.sub(r'/[^\\w\\s]/gi', '', x))\n",
    "test['string'] = test['string'].apply(lambda x: re.sub(r'[^\\u0000-\\u0800]', '', x))\n",
    "test = test[test['string'].apply(len) != 0] # remove empty strings\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to remove the rows with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20762, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a test dataset of 20828 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu',\n",
       "       'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test.language.unique()).issubset(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore all of the languages in the test dataset are in the training dataset, this is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Rank Similarity Methods\n",
    "\n",
    "I'm going to first try a [traditional method](http://odur.let.rug.nl/vannoord/TextCat/textcat.pdf) of ranked n-gram frequency similarity. I'll carry this out at the character level, since some of the test samples may be very short - we want to create a meaningful rank distribution.\n",
    "\n",
    "The authors in the linked paper use a simple rank similarity metric of the average distance between characters in the two ranks being compared. When comparing multiple languages, the language which produces the _smallest_ value of this metric is deemed the model's best prediction.\n",
    "\n",
    "It is unclear how the authors deal with n-grams which aren't mutually contained in the two ranks being compared. They simply write \"no-match = max\" which I've interpreted as setting no-matches equal to the max distance, which is equal to the length of the rank. The authors also report good classification for rank lengths as short as 200, so I shall use this rank length (perhaps experimenting later on). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sim(a, b):\n",
    "    '''\n",
    "    Takes two ranked arrays of the same shape and returns the average distance between the indices of matching elements\n",
    "    Elements which don't match are given a distance of the length of the array (this will perform poorly on short strings)\n",
    "    '''\n",
    "    c = a[np.where(np.in1d(a,b))[0]]\n",
    "    d = c.reshape((len(c),1))\n",
    "    e = np.abs(np.where(np.in1d(a,b))[0]-np.where(d==b)[1])\n",
    "    no_match_penalty = (len(a)-len(e))*len(a) # TODO: make this condition more rigourous\n",
    "    \n",
    "    return (e.sum() + no_match_penalty)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(corpus, ngram_range=(1,3), rank_length=200):\n",
    "    '''\n",
    "    Inputs:\n",
    "    corpus - a list of strings\n",
    "    ngram_range - the range of ngram lengths to consider (defaults to 1-3)\n",
    "    rank_length - the length of the rank list to return (defaults to 200)\n",
    "    \n",
    "    Retuns:\n",
    "    rank - the array of ngrams ordered by frequency of occurrence in the corpus\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(input='content',ngram_range=(1,3), analyzer='char_wb')\n",
    "    transformed = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    sums = transformed.sum(axis=0)\n",
    "    \n",
    "    return np.array(features)[np.array(np.argsort(sums[0,:]))[0]][-rank_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the functions on some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovakian/Latvian similarity score: 32.56\n",
      "Slovakian/Slovakian similarity score: 9.72\n",
      "Latvian/Latvian similarity score: 10.12\n"
     ]
    }
   ],
   "source": [
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])\n",
    "\n",
    "rank_sl_1 = rank(corpus_sl_1)\n",
    "rank_sl_2 = rank(corpus_sl_2)\n",
    "rank_lv_1 = rank(corpus_lv_1)\n",
    "rank_lv_2 = rank(corpus_lv_2)\n",
    "\n",
    "print('Slovakian/Latvian similarity score:', rank_sim(rank_sl_1, rank_lv_1))\n",
    "print('Slovakian/Slovakian similarity score:', rank_sim(rank_sl_1, rank_sl_2))\n",
    "print('Latvian/Latvian similarity score:', rank_sim(rank_lv_1, rank_lv_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both languages show similarity to themselves (a low score means a stronger agreement between their ranks), and less similarity between each other. In this simple binary classification, the two languages would have been told apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we will need to classify a given sample of text are the ranked ngrams for each language after analysing the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = next(os.walk('./txt'))[1]\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bd90b6131f4f3d98ac25ef83cd68f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranks = {}\n",
    "\n",
    "for language in tqdm(languages):\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    lang_corpus = corpus(file_list[:500])\n",
    "    lang_rank = rank(lang_corpus, rank_length=100)\n",
    "    ranks[language] = lang_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiom_check(lang_sample, ranks, print_scores=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "    lang_sample - the string to be classified\n",
    "    ranks - the dict() containing the language keys and their corresponding ranks\n",
    "    print_scores - bool - if True, the scores for each separate language is printed\n",
    "    \n",
    "    Returns:\n",
    "    The predicted language of lang_sample\n",
    "    '''\n",
    "    scores = {}\n",
    "    \n",
    "    sample_rank = rank([lang_sample])\n",
    "    \n",
    "    for key, rk in ranks.items():\n",
    "        scores[key] = rank_sim(sample_rank, rk)\n",
    "        \n",
    "    if print_scores:\n",
    "        print(scores)\n",
    "\n",
    "    return min(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test.copy()[:10000]\n",
    "results['pred_lang'] = results['string'].apply(lambda x: idiom_check(x, ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',(results['language'] == results['pred_lang']).sum()/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't bad, but we could do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain MLE\n",
    "\n",
    "The n-gram approach works fine, but isn't very well suited to short strings which don't contain a large number of unique n-grams. I believe a character-level Markov chain approach could perform better on shorter phrases because it doesn't have this limitation. \n",
    "\n",
    "I aim to treat each string as a first-order Markov chain and then extract the character-level transition matrix for each language. Then I can calculate the $\\log(probability)$ for a particular string to belong to each language and choose the maximally likely option. Something along the lines of [this](https://pdfs.semanticscholar.org/2bf0/8addb83f51befa8b4bc7ed16b54ed34018d0.pdf) I suppose. I have chosen to ignore the initial character probabilities, as their contribution to the final result is likely to be very small. However, this means that what I construct here is not technically a true Markov chain.\n",
    "\n",
    "There don't seem to be any Python packages for markov chains(?!) I will have to code this myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From - stackoverflow.com/a/43413801\n",
    "\n",
    "def strided_axis0(a, L):\n",
    "    # Store the shape and strides info\n",
    "    shp = a.shape\n",
    "    s  = a.strides\n",
    "\n",
    "    # Compute length of output array along the first axis\n",
    "    nd0 = shp[0]-L+1\n",
    "\n",
    "    # Setup shape and strides for use with np.lib.stride_tricks.as_strided\n",
    "    # and get (n+1) dim output array\n",
    "    shp_in = (nd0,L)+shp[1:]\n",
    "    strd_in = (s[0],) + s\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shp_in, strides=strd_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the transitions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'h'],\n",
       "       ['h', 'i'],\n",
       "       ['i', 's'],\n",
       "       ['s', ' '],\n",
       "       [' ', 'i'],\n",
       "       ['i', 's'],\n",
       "       ['s', ' '],\n",
       "       [' ', 'a'],\n",
       "       ['a', ' '],\n",
       "       [' ', 't']], dtype='<U1')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = \"This is a test\"\n",
    "test_arr = np.array(list(trial))\n",
    "pairs = strided_axis0(test_arr, 2)\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strided_axis0(test_arr, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(strided_axis0(test_arr, 2), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are only 11 unique transitions out of 13 measured transitions - this is how we will build our transition matrix.\n",
    "\n",
    "Our transition matrix could end up being quite large. We already have over 16,000 entries for the 128 characters in US-ASCII, if we extend this to the Greek characters then this could be around 4,000,000 entries.\n",
    "\n",
    "It's probably best to store these as a (vectorised) numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_encode(groups, encoder):\n",
    "    '''\n",
    "    Input:\n",
    "    groups - array - groups of characters to be encoded\n",
    "    encoder - a sklearn.preprocessing.LabelEncoder object which has been prefitted to a vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    int_encoded - array - the integer encoded groups\n",
    "    '''\n",
    "    flat_groups = groups.flatten()\n",
    "    try:\n",
    "        int_encoded = encoder.transform(flat_groups).reshape(groups.shape)\n",
    "    except ValueError:\n",
    "        print('Error:',''.join(list(groups[:,1])))\n",
    "    \n",
    "    return int_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work out a way of dealing with the zero-valued probabilities - these are events that aren't in the training data, but could be seen in the test data. For this it seems we should use some kind of [smoothing](https://pdfs.semanticscholar.org/5b2b/78087e51641a02966d6dcf20b51a5c43ccca.pdf). I should use _absolute discounting_ as it is easy to implement and apparently quite effective (would ideally use _Kneser-Ney smoothing_ but this would be very involved). [Here](http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf) is a good guide.\n",
    "\n",
    "However, this still seems too time-consuming. It may suffice to add a very small amount of probability to the zero values, we'll see when we test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(trans_mat):\n",
    "    '''\n",
    "    Smooths zero values with a small number << 1 (an altered form of Laplace smoothing)\n",
    "    \n",
    "    TODO: turn this into Kneser-Ney smoothing\n",
    "    \n",
    "    '''\n",
    "    smooth_prob = trans_mat[(trans_mat != 0) & ~np.isnan(trans_mat)].min()/10 # take the minimum probability and choose something smaller than it\n",
    "    trans_mat[trans_mat == 0] = smooth_prob\n",
    "    trans_mat[np.isnan(trans_mat)] = smooth_prob\n",
    "    \n",
    "    return trans_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(text_list, vocab, encoder):\n",
    "    '''\n",
    "    Input:\n",
    "    text_list - list of strings - the texts to analyse\n",
    "    \n",
    "    Returns:\n",
    "    trans_mat - array - the Markovian transition matrix for the text given\n",
    "    vocab - array - an array where the index of each item corresponds to the position in trans_mat: a lookup\n",
    "    '''\n",
    "    count_mat_master = np.zeros((vocab.shape[0],vocab.shape[0])) # intialise the master count matrix with zero counts\n",
    "    \n",
    "    for text in tqdm(text_list):\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "        text = np.array(list(text)) # prepare text as array of separate characters\n",
    "        pairs = strided_axis0(text, 2) # window characters into consecutive pairs\n",
    "        int_pairs = int_encode(pairs, encoder) # integer encode the characters\n",
    "        unique, counts = np.unique(int_pairs, return_counts=True, axis=0) # count the separate instances of the transitions\n",
    "        count_mat = np.zeros((vocab.shape[0],vocab.shape[0]))\n",
    "        count_mat[unique[:,0],unique[:,1]] = counts # populate the count matrix\n",
    "        count_mat_master += count_mat # add the counts to the master count matrix\n",
    "        \n",
    "    count_mat_master = smooth(count_mat_master)\n",
    "    trans_mat = count_mat_master/count_mat_master.sum(axis=1).reshape(count_mat_master.shape[0],1) # normalise the transition matrix (row stochastic)\n",
    "    \n",
    "    return trans_mat, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like(text, TM_list, encoder):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text - string - the sample to be analysed\n",
    "    TM_list - array - the list of transition matrices containing the transition probabilities for the calculation for each langauge\n",
    "    vocab - array - the characters corresponding to the transition matrix entries\n",
    "    \n",
    "    Returns:\n",
    "    log_likelihood - float - the log-likelihood for the string\n",
    "    \n",
    "    '''\n",
    "    text = np.array(list(text))\n",
    "    pairs = strided_axis0(text, 2)\n",
    "    int_pairs = int_encode(pairs, encoder)\n",
    "    prob_lists = np.zeros((TM_list.shape[0], int_pairs.shape[0]))\n",
    "    \n",
    "    for i, TM in enumerate(TM_list):\n",
    "        prob_lists[i] = TM[int_pairs[:,0],int_pairs[:,1]]\n",
    "    \n",
    "    return np.log(prob_lists).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to extract my vocab from the training data, or I could create a sample of every character that's possible to create with two bytes of UTF-8? Let's try using the first $128 + 1920 = 2,048$ characters. We have filtered to these characters in our text preprocessing anyway. This will be a very sparse matrix, but not too large for our means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([chr(i) for i in range(2048)])\n",
    "encoder = LabelEncoder().fit(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Test\n",
    "\n",
    "Let's test the routines on our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))\n",
    "\n",
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de83c359b2d44a74a6aa104176f2d25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18f91e35a454cfbad168c0e4280c48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TM_list = np.array([transition_matrix(corpus, vocab, encoder)[0] for corpus in [corpus_sl_1, corpus_lv_1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likes = np.zeros((len(corpus_sl_2),len(TM_list)))\n",
    "for i, sample in enumerate(corpus_sl_2):\n",
    "    log_likes[i,:] = log_like(sample, TM_list, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary detection accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "percentage_accuracy = 100*(np.where(log_likes[:,0] > log_likes[:,1])[0].shape[0] / log_likes.shape[0])\n",
    "print(\"Binary detection accuracy: %d%%\" % percentage_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model does indeed produce a larger log likelihood for the correct language, this is great. Now I need to create transition matrices for all of the languages and evaluate the model on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Markov Chain MLE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_list = np.zeros((len(languages), len(vocab), len(vocab)))\n",
    "\n",
    "for i, language in enumerate(tqdm(languages)):\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    a_corpus = corpus(file_list[:500])\n",
    "    TM_list[i] = transition_matrix(a_corpus, vocab, encoder)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the performance on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiom_check_mkv(lang_sample, TM_list, languages, encoder):\n",
    "    '''\n",
    "    Inputs:\n",
    "    lang_sample - the string to be classified\n",
    "    TM_list - array containing the transition matrices for each language\n",
    "    languages - list - list of the two character strings representing each language\n",
    "    encoder - sklearn.preprocessing.LabelEncoder obj - prefitted encoder\n",
    "    \n",
    "    Returns:\n",
    "    The predicted language of lang_sample - string\n",
    "    '''\n",
    "\n",
    "    return languages[np.argmax(log_like(lang_sample, TM_list, encoder))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test.copy()\n",
    "results['pred_lang'] = results['string'].apply(lambda x: idiom_check_mkv(x, TM_list, languages, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9707638955784607\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',(results['language'] == results['pred_lang']).sum()/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97% is quite good. I think at this point improvements would have to come either by using a larger sample for the transition matrices, or by improving on the smoothing technique (which is very rudimentary as it stands). Let's look to see where it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     607\n",
       "unique     18\n",
       "top        fi\n",
       "freq      345\n",
       "Name: pred_lang, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(results['language'] != results['pred_lang'])]['pred_lang'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>et</td>\n",
       "      <td>Sellise lähenemisega jäämegi ebakindlasse oluk...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>336</td>\n",
       "      <td>1</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       language                                             string pred_lang\n",
       "count       345                                                345       345\n",
       "unique        4                                                345         1\n",
       "top          et  Sellise lähenemisega jäämegi ebakindlasse oluk...        fi\n",
       "freq        336                                                  1       345"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(results['language'] != results['pred_lang']) & (results['pred_lang'] == 'fi')].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems almost all of the samples which were mistaken for Finnish were actually Estonian. These languages must be quite similar in terms of their character transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>et</td>\n",
       "      <td>Austatud juhataja! Tahaksin kõigepealt avalda...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>et</td>\n",
       "      <td>Euroopa Ülemkogu on väljendanud lootust, et v...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>et</td>\n",
       "      <td>Härra juhataja, me oleme rääkinud üleilmastum...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>et</td>\n",
       "      <td>Härra president, volinik, daamid ja härrad, h...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>et</td>\n",
       "      <td>Proua juhataja, Ria Oomen-Ruijten on esitanud...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                             string pred_lang\n",
       "6993       et   Austatud juhataja! Tahaksin kõigepealt avalda...        et\n",
       "6994       et   Euroopa Ülemkogu on väljendanud lootust, et v...        et\n",
       "6995       et   Härra juhataja, me oleme rääkinud üleilmastum...        et\n",
       "6996       et   Härra president, volinik, daamid ja härrad, h...        et\n",
       "6997       et   Proua juhataja, Ria Oomen-Ruijten on esitanud...        fi"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['language'] == 'et'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, haluaisin äänestää Hannes S...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, minulla on kaksi lisäkysymy...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, poikkean käsikirjoituksestani.</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, ennen kuin arvostelemme Ven...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>fi</td>\n",
       "      <td>Arvoisa puhemies, haluan kiittää näiden kahde...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                             string pred_lang\n",
       "7992       fi   Arvoisa puhemies, haluaisin äänestää Hannes S...        fi\n",
       "7993       fi   Arvoisa puhemies, minulla on kaksi lisäkysymy...        fi\n",
       "7994       fi   Arvoisa puhemies, poikkean käsikirjoituksestani.        fi\n",
       "7995       fi   Arvoisa puhemies, ennen kuin arvostelemme Ven...        fi\n",
       "7996       fi   Arvoisa puhemies, haluan kiittää näiden kahde...        fi"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['language'] == 'fi'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce the confusion matrix for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted   bg    cs    da   de   el    en    es   et    fi    fr   ...     \\\n",
      "Actual                                                              ...      \n",
      "bg         997     0     0    0    0     0     0    0     0     0   ...      \n",
      "cs           0   975     1    0    1     1     2    0     1     0   ...      \n",
      "da           0     0   991    0    0     0     0    0     0     0   ...      \n",
      "de           0     0     1  992    1     0     0    0     0     0   ...      \n",
      "el           0     0     0    0  988     0     0    0     0     0   ...      \n",
      "en           0     0     0    0    0   998     0    0     0     0   ...      \n",
      "es           0     0     0    0    0     0   994    0     0     0   ...      \n",
      "et           0     0     0    3    0     4     0  617   336     0   ...      \n",
      "fi           0     0     0    0    0     0     0    0   995     0   ...      \n",
      "fr           0     0     0    0    0     0     1    0     0   996   ...      \n",
      "hu           1     0     5    0    0     0     0    0     0     0   ...      \n",
      "it           0     0     0    1    0     0     0    0     0     0   ...      \n",
      "lt           0     0     3    1    0     1     1    0     5     1   ...      \n",
      "lv           0     0     0    0    0     1     0    0     0     0   ...      \n",
      "nl           0     0     0    0    0     0     0    0     0     0   ...      \n",
      "pl           0     0     0    1    1     1     0    0     0     0   ...      \n",
      "pt           0     0     0    0    0     0     0    0     0     0   ...      \n",
      "ro           0     0     0    0    0     1     4    0     0    14   ...      \n",
      "sk           0    43     1    0    0     0     6    0     0     0   ...      \n",
      "sl           0     1    13    1    0     0     6    0     3     0   ...      \n",
      "sv           0     0     4    0    0     0     0    0     0     0   ...      \n",
      "__all__    998  1019  1019  999  991  1007  1014  617  1340  1011   ...      \n",
      "\n",
      "Predicted   lt   lv    nl   pl    pt   ro   sk   sl    sv  __all__  \n",
      "Actual                                                              \n",
      "bg           0    0     0    0     0    0    0    0     0      997  \n",
      "cs           0    0     1    0     2    0    8    1     0      993  \n",
      "da           0    0     0    0     0    0    0    0     3      994  \n",
      "de           0    0     0    0     0    0    0    0     0      994  \n",
      "el           0    0     0    0     0    0    0    0     0      988  \n",
      "en           0    0     1    0     0    0    0    0     0      999  \n",
      "es           0    0     0    0     2    0    0    0     0      997  \n",
      "et           0    0    18    0     1    0    0    0    14      994  \n",
      "fi           0    0     0    0     0    0    0    0     0      995  \n",
      "fr           0    0     0    0     0    0    0    0     0      999  \n",
      "hu           0    0     2    0     2    0    0    0     1      998  \n",
      "it           0    0     0    0     0    0    0    0     0      996  \n",
      "lt         977    2     0    0     2    0    0    1     0      995  \n",
      "lv           1  973     1    0     2    0    0    0     0      978  \n",
      "nl           0    0   999    0     0    0    0    0     0      999  \n",
      "pl           0    0     0  996     0    0    0    0     0      999  \n",
      "pt           0    0     0    0   996    0    0    0     0      996  \n",
      "ro           0    0     0    0     5  894    0    0     0      928  \n",
      "sk           1    0     0    0     5    0  869    1     2      929  \n",
      "sl           0    0     2    2    10    0    3  934    13      998  \n",
      "sv           0    0     0    0     0    0    0    0   992      996  \n",
      "__all__    979  975  1024  998  1027  894  880  937  1025    20762  \n",
      "\n",
      "[22 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = ConfusionMatrix(results['language'], results['pred_lang'])\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are generally good apart from a very poor performance for Estonian. Interestingly, Estonian is mistaken for Finnish, but not vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estonian Accuracy: 0.6207243460764588\n"
     ]
    }
   ],
   "source": [
    "et_results = results[results['language'] == 'et']\n",
    "print('Estonian Accuracy:',((et_results['language'] == et_results['pred_lang'])).sum()/len(et_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "I'm going to try training a simple LSTM model on the language data using TensorFlow to see if I can get even better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from utils import makeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([chr(i) for i in range(2048)])\n",
    "encoder = LabelEncoder().fit(vocab)\n",
    "\n",
    "languages = next(os.walk('./txt'))[1]\n",
    "language_encoder = LabelEncoder().fit(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Let's process our data so it can be read by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_per_lang = 1000\n",
    "\n",
    "# Train data\n",
    "train = makeDF(os.getcwd(),sequences_per_lang)\n",
    "train['as_numbers'] = train['string'].apply(lambda x: encoder.transform(list(x)))\n",
    "train['length'] = train['string'].apply(lambda x: len(x))\n",
    "train['lang_as_numbers'] = language_encoder.transform(train['language'])\n",
    "\n",
    "# Test data\n",
    "test = pd.read_csv('europarl-test.txt', sep='\\t', header=None, names=['language', 'string'])\n",
    "test['string'] = test['string'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "test['string'] = test['string'].str.replace(\"'\",\"\")\n",
    "test['string'] = test['string'].apply(lambda x: re.sub(r'/[^\\w\\s]/gi', '', x))\n",
    "test['string'] = test['string'].apply(lambda x: re.sub(r'[^\\u0000-\\u0800]', '', x))\n",
    "test = test[test['string'].apply(len) != 0] # remove empty strings\n",
    "test['as_numbers'] = test['string'].apply(lambda x: encoder.transform(list(x)))\n",
    "test['length'] = test['string'].apply(lambda x: len(x))\n",
    "test['lang_as_numbers'] = language_encoder.transform(test['language'])\n",
    "\n",
    "# Validation/test split\n",
    "test_len = len(test)\n",
    "rand_inds = np.random.choice(test_len,test_len)\n",
    "test = test.loc[rand_inds[:test_len//2],:]\n",
    "val = test.loc[rand_inds[test_len//2]:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JBremner/Envs/idiom/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "test_len = len(test)\n",
    "rand_inds = np.random.choice(test_len,test_len)\n",
    "test = test.iloc[rand_inds[test_len//2:]]\n",
    "val = test.loc[rand_inds[:test_len//2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1298, 5)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1325, 5)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddedDataIterator():\n",
    "    def __init__(self, df, maxlen):\n",
    "        self.df = df\n",
    "        self.rm_too_short()\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.maxlen = maxlen\n",
    "        self.shuffle()\n",
    "        \n",
    "    def rm_too_short(self): # remove sequences with fewer items than maxlen\n",
    "        self.df = self.df.drop(self.df[self.df['length'] < maxlen].index).reset_index(drop=True)\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.loc[self.cursor:self.cursor+n-1]\n",
    "        if len(res) != n:\n",
    "            print(res)\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        # maxlen = max(res['length'])\n",
    "        maxlen = self.maxlen\n",
    "        x = pad_sequences(res['as_numbers'].values, maxlen, padding='post', truncating='post')\n",
    "\n",
    "        return x, res['lang_as_numbers'].values, np.array([maxlen]*x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def build_graph(\n",
    "    vocab_size = len(vocab),\n",
    "    state_size = 64,\n",
    "    batch_size = 32,\n",
    "    num_classes = len(languages),\n",
    "    learning_rate = 1e-3):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, shape=[batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    y = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    keep_prob = tf.placeholder(tf.float32,[])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', shape=[vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # LSTM\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(state_size)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=keep_prob)\n",
    "    rnn_outputs, _ = tf.nn.dynamic_rnn(lstmCell, rnn_inputs, dtype=tf.float32)\n",
    "    \n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'keep_prob': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 32, num_epochs = 10, iterator = PaddedDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        maxlen = 64\n",
    "        keep_prob = 0.6\n",
    "        tr = iterator(train, maxlen)\n",
    "        te = iterator(test, maxlen)\n",
    "        g = graph\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['keep_prob']: keep_prob}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['keep_prob']: keep_prob}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - acc:\", tr_losses[-1], \"- val_acc:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JBremner/Envs/idiom/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - acc: 0.19503583473861721 - val_acc: 0.25636848341232227\n",
      "Accuracy after epoch 2  - acc: 0.33213682432432434 - val_acc: 0.38924050632911394\n",
      "Accuracy after epoch 3  - acc: 0.46230996621621623 - val_acc: 0.48392998417721517\n",
      "Accuracy after epoch 4  - acc: 0.5580130912162162 - val_acc: 0.5363924050632911\n",
      "Accuracy after epoch 5  - acc: 0.6628483952702703 - val_acc: 0.6620846518987342\n",
      "Accuracy after epoch 6  - acc: 0.7321579391891891 - val_acc: 0.7166732594936709\n",
      "Accuracy after epoch 7  - acc: 0.7807749155405406 - val_acc: 0.7176621835443038\n",
      "Accuracy after epoch 8  - acc: 0.7838893581081081 - val_acc: 0.7298259493670886\n",
      "Accuracy after epoch 9  - acc: 0.8155088682432432 - val_acc: 0.7724485759493671\n",
      "Accuracy after epoch 10  - acc: 0.8553103885135135 - val_acc: 0.785996835443038\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Absolute Discounting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
