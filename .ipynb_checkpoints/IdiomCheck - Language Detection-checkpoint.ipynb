{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IdiomCheck - Language Detection\n",
    "\n",
    "Here I detail my attempts to create a language classifier trained on the [European Parliament Proceedings Parallel Corpus](http://www.statmt.org/europarl/) and tested using [this](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/language-detection/europarl-test.zip) dataset.\n",
    "\n",
    "I use three approaches:\n",
    "- N-gram frequency distribution comparison\n",
    "- Markov chain MLE\n",
    "- LSTM-based neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_cleaner(sample):\n",
    "    \n",
    "    sample = re.sub(r'[^\\u0000-\\u0800]', '', sample) # select only the first 2048 UTF-8 characters\n",
    "    sample = re.sub(r'\\([^)]*\\)', ' ', sample) # rm characters within brackets\n",
    "    sample = re.sub('<[^>]+>', '', sample) # rm characters within <>\n",
    "    sample = re.sub(r'\\d+', '', sample) # rm one or more digits\n",
    "    sample = sample.replace('\\n', ' ') # rm line delimiters\n",
    "    sample = re.sub(r'/[^\\w\\s]/gi', '', sample)\n",
    "    sample = re.sub(r'\\W+', ' ', sample) # rm non-word characters\n",
    "    sample = re.sub(\"'\", '', sample) # rm single quotes\n",
    "    sample = sample.lower() # make lowercase\n",
    "    sample = re.sub(r'^\\s','', sample) # rm space at start\n",
    "    sample = re.sub(r'\\s$','', sample) # rm space at end\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(file_list):\n",
    "    '''\n",
    "    Inputs:\n",
    "    file_list - the list of paths of the text files to create the corpus\n",
    "    \n",
    "    Returns:\n",
    "    A list of strings containing the corpus\n",
    "    '''\n",
    "    corpus = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        with open(file_path) as f_input:\n",
    "            sample = f_input.read()[:20000]\n",
    "            sample = string_cleaner(sample)\n",
    "            if len(sample) > 0:\n",
    "                corpus.append(sample)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Избор на квестори на Европейския парламент срок за депозиране на кандидатури: вж. протокола  Die Sitzung wird um 15.25 Uhr unterbrochen und um 18.00 Uhr wiederaufgenommen. '"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"bg\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])\n",
    "the_corpus[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Bulgarian sample above contains German (normally in brackets at the end), we need to remove this. Have altered `corpus()` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'избор на квестори на европейския парламент вж протокола'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"bg\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])\n",
    "the_corpus[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it has been removed, great. It has also removed all other text in brackets but this will be in a minority of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", \"cs\",\"*.txt\"))\n",
    "the_corpus = corpus(file_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schválení zápisu z předchozího zasedání viz zápis\n",
      "členství ve výborech a delegacích viz zápis\n",
      "předložení dokumentů viz zápis\n"
     ]
    }
   ],
   "source": [
    "print(the_corpus[0])\n",
    "print(the_corpus[1])\n",
    "print(the_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the early files contain the same ending which we'll need to strip as these will effect the frequency counts. Luckily each language has the same phrase after the colon in the same corresponding file. My guess is that this doesn't affect the overall statistics, given that it'll be lost amongst large text files. We can always come back and alter this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "The test data comes as a .tsv file with the first column representing the language and the second column containing the string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('europarl-test.txt', sep='\\t', header=None, names=['language', 'string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>(CS) Най-голямата несправедливост на сегашната...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-жо председател, г-н член на Комисията, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, бих искал да започна с к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, въпросът за правата на ч...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1       bg  (CS) Най-голямата несправедливост на сегашната...\n",
       "2       bg  (DE) Г-жо председател, г-н член на Комисията, ...\n",
       "3       bg  (DE) Г-н председател, бих искал да започна с к...\n",
       "4       bg  (DE) Г-н председател, въпросът за правата на ч..."
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to strip the '(CS)','(DE)' etc. As well as all the non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>европа не трябва да стартира нов конкурентен м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>най голямата несправедливост на сегашната обща...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>г жо председател г н член на комисията по прин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>г н председател бих искал да започна с комента...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>г н председател въпросът за правата на човека ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                             string\n",
       "0       bg  европа не трябва да стартира нов конкурентен м...\n",
       "1       bg  най голямата несправедливост на сегашната обща...\n",
       "2       bg  г жо председател г н член на комисията по прин...\n",
       "3       bg  г н председател бих искал да започна с комента...\n",
       "4       bg  г н председател въпросът за правата на човека ..."
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test['string'] = test['string'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "# test['string'] = test['string'].str.replace(\"'\",\"\")\n",
    "# test['string'] = test['string'].apply(lambda x: re.sub(r'/[^\\w\\s]/gi', '', x))\n",
    "# test['string'] = test['string'].apply(lambda x: re.sub(r'[^\\u0000-\\u0800]', '', x))\n",
    "test['string'] = test['string'].apply(lambda x: string_cleaner(x))\n",
    "test = test[test['string'].apply(len) != 0] # remove empty strings\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to remove the rows with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20762, 2)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a test dataset of 20828 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu',\n",
       "       'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test.language.unique()).issubset(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore all of the languages in the test dataset are in the training dataset, this is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Rank Similarity Methods\n",
    "\n",
    "I'm going to first try a [traditional method](http://odur.let.rug.nl/vannoord/TextCat/textcat.pdf) of ranked n-gram frequency similarity. I'll carry this out at the character level, since some of the test samples may be very short - we want to create a meaningful rank distribution.\n",
    "\n",
    "The authors in the linked paper use a simple rank similarity metric of the average distance between characters in the two ranks being compared. When comparing multiple languages, the language which produces the _smallest_ value of this metric is deemed the model's best prediction.\n",
    "\n",
    "It is unclear how the authors deal with n-grams which aren't mutually contained in the two ranks being compared. They simply write \"no-match = max\" which I've interpreted as setting no-matches equal to the max distance, which is equal to the length of the rank. The authors also report good classification for rank lengths as short as 200, so I shall use this rank length (perhaps experimenting later on). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sim(a, b):\n",
    "    '''\n",
    "    Takes two ranked arrays of the same shape and returns the average distance between the indices of matching elements\n",
    "    Elements which don't match are given a distance of the length of the array (this will perform poorly on short strings)\n",
    "    '''\n",
    "    c = a[np.where(np.in1d(a,b))[0]]\n",
    "    d = c.reshape((len(c),1))\n",
    "    e = np.abs(np.where(np.in1d(a,b))[0]-np.where(d==b)[1])\n",
    "    no_match_penalty = (len(a)-len(e))*len(a) # TODO: make this condition more rigourous\n",
    "    \n",
    "    return (e.sum() + no_match_penalty)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(corpus, ngram_range=(1,3), rank_length=200):\n",
    "    '''\n",
    "    Inputs:\n",
    "    corpus - a list of strings\n",
    "    ngram_range - the range of ngram lengths to consider (defaults to 1-3)\n",
    "    rank_length - the length of the rank list to return (defaults to 200)\n",
    "    \n",
    "    Retuns:\n",
    "    rank - the array of ngrams ordered by frequency of occurrence in the corpus\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(input='content',ngram_range=(1,3), analyzer='char_wb')\n",
    "    transformed = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    sums = transformed.sum(axis=0)\n",
    "    \n",
    "    return np.array(features)[np.array(np.argsort(sums[0,:]))[0]][-rank_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the functions on some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovakian/Latvian similarity score: 138.335\n",
      "Slovakian/Slovakian similarity score: 49.7\n",
      "Latvian/Latvian similarity score: 60.48\n"
     ]
    }
   ],
   "source": [
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])\n",
    "\n",
    "rank_sl_1 = rank(corpus_sl_1)\n",
    "rank_sl_2 = rank(corpus_sl_2)\n",
    "rank_lv_1 = rank(corpus_lv_1)\n",
    "rank_lv_2 = rank(corpus_lv_2)\n",
    "\n",
    "print('Slovakian/Latvian similarity score:', rank_sim(rank_sl_1, rank_lv_1))\n",
    "print('Slovakian/Slovakian similarity score:', rank_sim(rank_sl_1, rank_sl_2))\n",
    "print('Latvian/Latvian similarity score:', rank_sim(rank_lv_1, rank_lv_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both languages show similarity to themselves (a low score means a stronger agreement between their ranks), and less similarity between each other. In this simple binary classification, the two languages would have been told apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we will need to classify a given sample of text are the ranked ngrams for each language after analysing the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = next(os.walk('./txt'))[1]\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = {}\n",
    "\n",
    "for language in tqdm(languages):\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    lang_corpus = corpus(file_list[:500])\n",
    "    lang_rank = rank(lang_corpus, rank_length=100)\n",
    "    ranks[language] = lang_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiom_check(lang_sample, ranks, print_scores=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "    lang_sample - the string to be classified\n",
    "    ranks - the dict() containing the language keys and their corresponding ranks\n",
    "    print_scores - bool - if True, the scores for each separate language is printed\n",
    "    \n",
    "    Returns:\n",
    "    The predicted language of lang_sample\n",
    "    '''\n",
    "    scores = {}\n",
    "    \n",
    "    sample_rank = rank([lang_sample])\n",
    "    \n",
    "    for key, rk in ranks.items():\n",
    "        scores[key] = rank_sim(sample_rank, rk)\n",
    "        \n",
    "    if print_scores:\n",
    "        print(scores)\n",
    "\n",
    "    return min(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test.copy()[:10000]\n",
    "results['pred_lang'] = results['string'].apply(lambda x: idiom_check(x, ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8792\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',(results['language'] == results['pred_lang']).sum()/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't bad, but we could do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain MLE\n",
    "\n",
    "The n-gram approach works fine, but isn't very well suited to short strings which don't contain a large number of unique n-grams. I believe a character-level Markov chain approach could perform better on shorter phrases because it doesn't have this limitation. \n",
    "\n",
    "I aim to treat each string as a first-order Markov chain and then extract the character-level transition matrix for each language. Then I can calculate the $\\log(probability)$ for a particular string to belong to each language and choose the maximally likely option. Something along the lines of [this](https://pdfs.semanticscholar.org/2bf0/8addb83f51befa8b4bc7ed16b54ed34018d0.pdf) I suppose. I have chosen to ignore the initial character probabilities, as their contribution to the final result is likely to be very small. However, this means that what I construct here is not technically a true Markov chain.\n",
    "\n",
    "There don't seem to be any Python packages for markov chains(?!) I will have to code this myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From - stackoverflow.com/a/43413801\n",
    "\n",
    "def strided_axis0(a, L):\n",
    "    # Store the shape and strides info\n",
    "    shp = a.shape\n",
    "    s  = a.strides\n",
    "\n",
    "    # Compute length of output array along the first axis\n",
    "    nd0 = shp[0]-L+1\n",
    "\n",
    "    # Setup shape and strides for use with np.lib.stride_tricks.as_strided\n",
    "    # and get (n+1) dim output array\n",
    "    shp_in = (nd0,L)+shp[1:]\n",
    "    strd_in = (s[0],) + s\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shp_in, strides=strd_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the transitions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'h'],\n",
       "       ['h', 'i'],\n",
       "       ['i', 's'],\n",
       "       ['s', ' '],\n",
       "       [' ', 'i'],\n",
       "       ['i', 's'],\n",
       "       ['s', ' '],\n",
       "       [' ', 'a'],\n",
       "       ['a', ' '],\n",
       "       [' ', 't']], dtype='<U1')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = \"This is a test\"\n",
    "test_arr = np.array(list(trial))\n",
    "pairs = strided_axis0(test_arr, 2)\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strided_axis0(test_arr, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(strided_axis0(test_arr, 2), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are only 11 unique transitions out of 13 measured transitions - this is how we will build our transition matrix.\n",
    "\n",
    "Our transition matrix could end up being quite large. We already have over 16,000 entries for the 128 characters in US-ASCII, if we extend this to the Greek characters then this could be around 4,000,000 entries.\n",
    "\n",
    "It's probably best to store these as a (vectorised) numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_encode(groups, encoder):\n",
    "    '''\n",
    "    Input:\n",
    "    groups - array - groups of characters to be encoded\n",
    "    encoder - a sklearn.preprocessing.LabelEncoder object which has been prefitted to a vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    int_encoded - array - the integer encoded groups\n",
    "    '''\n",
    "    flat_groups = groups.flatten()\n",
    "    try:\n",
    "        int_encoded = encoder.transform(flat_groups).reshape(groups.shape)\n",
    "    except ValueError:\n",
    "        print('Error:',''.join(list(groups[:,1])))\n",
    "    \n",
    "    return int_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work out a way of dealing with the zero-valued probabilities - these are events that aren't in the training data, but could be seen in the test data. For this it seems we should use some kind of [smoothing](https://pdfs.semanticscholar.org/5b2b/78087e51641a02966d6dcf20b51a5c43ccca.pdf). I should use _absolute discounting_ as it is easy to implement and apparently quite effective (would ideally use _Kneser-Ney smoothing_ but this would be very involved). [Here](http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf) is a good guide.\n",
    "\n",
    "However, this still seems too time-consuming. It may suffice to add a very small amount of probability to the zero values, we'll see when we test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(trans_mat):\n",
    "    '''\n",
    "    Smooths zero values with a small number << 1 (an altered form of Laplace smoothing)\n",
    "    \n",
    "    TODO: turn this into Kneser-Ney smoothing\n",
    "    \n",
    "    '''\n",
    "    smooth_prob = trans_mat[(trans_mat != 0) & ~np.isnan(trans_mat)].min()/10 # take the minimum probability and choose something smaller than it\n",
    "    trans_mat[trans_mat == 0] = smooth_prob\n",
    "    trans_mat[np.isnan(trans_mat)] = smooth_prob\n",
    "    \n",
    "    return trans_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(text_list, vocab, encoder):\n",
    "    '''\n",
    "    Input:\n",
    "    text_list - list of strings - the texts to analyse\n",
    "    \n",
    "    Returns:\n",
    "    trans_mat - array - the Markovian transition matrix for the text given\n",
    "    vocab - array - an array where the index of each item corresponds to the position in trans_mat: a lookup\n",
    "    '''\n",
    "    count_mat_master = np.zeros((vocab.shape[0],vocab.shape[0])) # intialise the master count matrix with zero counts\n",
    "    \n",
    "    for text in tqdm(text_list):\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "        text = np.array(list(text)) # prepare text as array of separate characters\n",
    "        pairs = strided_axis0(text, 2) # window characters into consecutive pairs\n",
    "        int_pairs = int_encode(pairs, encoder) # integer encode the characters\n",
    "        unique, counts = np.unique(int_pairs, return_counts=True, axis=0) # count the separate instances of the transitions\n",
    "        count_mat = np.zeros((vocab.shape[0],vocab.shape[0]))\n",
    "        count_mat[unique[:,0],unique[:,1]] = counts # populate the count matrix\n",
    "        count_mat_master += count_mat # add the counts to the master count matrix\n",
    "        \n",
    "    count_mat_master = smooth(count_mat_master)\n",
    "    trans_mat = count_mat_master/count_mat_master.sum(axis=1).reshape(count_mat_master.shape[0],1) # normalise the transition matrix (row stochastic)\n",
    "    \n",
    "    return trans_mat, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like(text, TM_list, encoder):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text - string - the sample to be analysed\n",
    "    TM_list - array - the list of transition matrices containing the transition probabilities for the calculation for each langauge\n",
    "    vocab - array - the characters corresponding to the transition matrix entries\n",
    "    \n",
    "    Returns:\n",
    "    log_likelihood - float - the log-likelihood for the string\n",
    "    \n",
    "    '''\n",
    "    text = np.array(list(text))\n",
    "    pairs = strided_axis0(text, 2)\n",
    "    int_pairs = int_encode(pairs, encoder)\n",
    "    prob_lists = np.zeros((TM_list.shape[0], int_pairs.shape[0]))\n",
    "    \n",
    "    for i, TM in enumerate(TM_list):\n",
    "        prob_lists[i] = TM[int_pairs[:,0],int_pairs[:,1]]\n",
    "    \n",
    "    return np.log(prob_lists).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to extract my vocab from the training data, or I could create a sample of every character that's possible to create with two bytes of UTF-8? Let's try using the first $128 + 1920 = 2,048$ characters. We have filtered to these characters in our text preprocessing anyway. This will be a very sparse matrix, but not too large for our means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([chr(i) for i in range(2048)])\n",
    "encoder = LabelEncoder().fit(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Test\n",
    "\n",
    "Let's test the routines on our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_sl = glob.glob(os.path.join(os.getcwd(), \"txt\", \"sl\",\"*.txt\"))\n",
    "file_list_lv = glob.glob(os.path.join(os.getcwd(), \"txt\", \"lv\",\"*.txt\"))\n",
    "\n",
    "corpus_sl_1 = corpus(file_list_sl[:50])\n",
    "corpus_sl_2 = corpus(file_list_sl[50:100])\n",
    "corpus_lv_1 = corpus(file_list_lv[:50])\n",
    "corpus_lv_2 = corpus(file_list_lv[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_list = np.array([transition_matrix(corpus, vocab, encoder)[0] for corpus in [corpus_sl_1, corpus_lv_1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likes = np.zeros((len(corpus_sl_2),len(TM_list)))\n",
    "for i, sample in enumerate(corpus_sl_2):\n",
    "    log_likes[i,:] = log_like(sample, TM_list, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary detection accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "percentage_accuracy = 100*(np.where(log_likes[:,0] > log_likes[:,1])[0].shape[0] / log_likes.shape[0])\n",
    "print(\"Binary detection accuracy: %d%%\" % percentage_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model does indeed produce a larger log likelihood for the correct language, this is great. Now I need to create transition matrices for all of the languages and evaluate the model on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Markov Chain MLE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_list = np.zeros((len(languages), len(vocab), len(vocab)))\n",
    "\n",
    "for i, language in enumerate(tqdm(languages)):\n",
    "    file_list = glob.glob(os.path.join(os.getcwd(), \"txt\", language,\"*.txt\"))\n",
    "    a_corpus = corpus(file_list[:500])\n",
    "    TM_list[i] = transition_matrix(a_corpus, vocab, encoder)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the performance on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiom_check_mkv(lang_sample, TM_list, languages, encoder):\n",
    "    '''\n",
    "    Inputs:\n",
    "    lang_sample - the string to be classified\n",
    "    TM_list - array containing the transition matrices for each language\n",
    "    languages - list - list of the two character strings representing each language\n",
    "    encoder - sklearn.preprocessing.LabelEncoder obj - prefitted encoder\n",
    "    \n",
    "    Returns:\n",
    "    The predicted language of lang_sample - string\n",
    "    '''\n",
    "\n",
    "    return languages[np.argmax(log_like(lang_sample, TM_list, encoder))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test.copy()\n",
    "results['pred_lang'] = results['string'].apply(lambda x: idiom_check_mkv(x, TM_list, languages, encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9855505249975918\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',(results['language'] == results['pred_lang']).sum()/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.5% is quite good. I think at this point improvements would have to come either by using a larger sample for the transition matrices, or by improving on the smoothing technique (which is very rudimentary as it stands). Let's look to see where it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     300\n",
       "unique     17\n",
       "top        fi\n",
       "freq      140\n",
       "Name: pred_lang, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(results['language'] != results['pred_lang'])]['pred_lang'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>et</td>\n",
       "      <td>ma kuulasin mis president barrosol öelda oli</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       language                                        string pred_lang\n",
       "count       140                                           140       140\n",
       "unique        2                                           140         1\n",
       "top          et  ma kuulasin mis president barrosol öelda oli        fi\n",
       "freq        137                                             1       140"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(results['language'] != results['pred_lang']) & (results['pred_lang'] == 'fi')].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems almost all of the samples which were mistaken for Finnish were actually Estonian. These languages must be quite similar in terms of their character transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>et</td>\n",
       "      <td>austatud juhataja tahaksin kõigepealt avaldada...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>et</td>\n",
       "      <td>euroopa ülemkogu on väljendanud lootust et või...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>et</td>\n",
       "      <td>härra juhataja me oleme rääkinud üleilmastumis...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>et</td>\n",
       "      <td>härra president volinik daamid ja härrad hilju...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>et</td>\n",
       "      <td>proua juhataja ria oomen ruijten on esitanud t...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                             string pred_lang\n",
       "6993       et  austatud juhataja tahaksin kõigepealt avaldada...        et\n",
       "6994       et  euroopa ülemkogu on väljendanud lootust et või...        et\n",
       "6995       et  härra juhataja me oleme rääkinud üleilmastumis...        et\n",
       "6996       et  härra president volinik daamid ja härrad hilju...        et\n",
       "6997       et  proua juhataja ria oomen ruijten on esitanud t...        fi"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['language'] == 'et'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>string</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>fi</td>\n",
       "      <td>arvoisa puhemies haluaisin äänestää hannes swo...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>fi</td>\n",
       "      <td>arvoisa puhemies minulla on kaksi lisäkysymyst...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>fi</td>\n",
       "      <td>arvoisa puhemies poikkean käsikirjoituksestani</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fi</td>\n",
       "      <td>arvoisa puhemies ennen kuin arvostelemme venäj...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>fi</td>\n",
       "      <td>arvoisa puhemies haluan kiittää näiden kahden ...</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                             string pred_lang\n",
       "7992       fi  arvoisa puhemies haluaisin äänestää hannes swo...        fi\n",
       "7993       fi  arvoisa puhemies minulla on kaksi lisäkysymyst...        fi\n",
       "7994       fi     arvoisa puhemies poikkean käsikirjoituksestani        fi\n",
       "7995       fi  arvoisa puhemies ennen kuin arvostelemme venäj...        fi\n",
       "7996       fi  arvoisa puhemies haluan kiittää näiden kahden ...        fi"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['language'] == 'fi'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce the confusion matrix for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted   bg    cs   da    de   el    en    es   et    fi   fr   ...     \\\n",
      "Actual                                                             ...      \n",
      "bg         997     0    0     0    0     0     0    0     0    0   ...      \n",
      "cs           0   978    0     0    0     2     0    0     0    0   ...      \n",
      "da           0     0  989     0    0     0     0    0     0    0   ...      \n",
      "de           0     0    0   991    0     1     0    0     0    0   ...      \n",
      "el           0     0    0     0  988     0     0    0     0    0   ...      \n",
      "en           0     0    0     0    0   999     0    0     0    0   ...      \n",
      "es           0     0    0     0    0     0   993    0     0    0   ...      \n",
      "et           0     0    0     5    0     1     0  836   137    0   ...      \n",
      "fi           0     0    0     0    0     0     0    0   995    0   ...      \n",
      "fr           0     0    0     1    0     1     1    0     0  993   ...      \n",
      "hu           0     0    2     1    0     0     0    0     0    0   ...      \n",
      "it           0     0    0     0    0     0     1    0     0    0   ...      \n",
      "lt           0     0    1     0    0     1     1    0     3    0   ...      \n",
      "lv           0     0    0     0    0     0     0    0     0    0   ...      \n",
      "nl           0     0    1     2    0     0     0    0     0    0   ...      \n",
      "pl           0     1    0     1    0     1     0    0     0    0   ...      \n",
      "pt           0     0    0     0    0     1     0    0     0    0   ...      \n",
      "ro           0     0    0     0    0     1     0    0     0    4   ...      \n",
      "sk           0    31    0     0    0     0     1    0     0    0   ...      \n",
      "sl           0     1    1     0    0     0     5    0     0    0   ...      \n",
      "sv           0     0    3     0    0     0     0    0     0    0   ...      \n",
      "__all__    997  1011  997  1001  988  1008  1002  836  1135  997   ...      \n",
      "\n",
      "Predicted   lt   lv    nl   pl    pt   ro   sk   sl    sv  __all__  \n",
      "Actual                                                              \n",
      "bg           0    0     0    0     0    0    0    0     0      997  \n",
      "cs           0    0     0    0     1    0   11    0     0      993  \n",
      "da           0    0     0    0     0    0    0    0     5      994  \n",
      "de           0    0     2    0     0    0    0    0     0      994  \n",
      "el           0    0     0    0     0    0    0    0     0      988  \n",
      "en           0    0     0    0     0    0    0    0     0      999  \n",
      "es           0    0     0    0     3    0    0    0     0      997  \n",
      "et           0    0     7    0     0    0    0    0     8      994  \n",
      "fi           0    0     0    0     0    0    0    0     0      995  \n",
      "fr           0    0     0    0     1    0    0    0     0      999  \n",
      "hu           0    0     1    0     1    0    0    0     3      998  \n",
      "it           0    0     0    0     0    1    0    0     0      996  \n",
      "lt         987    1     0    0     0    0    0    0     0      995  \n",
      "lv           0  976     1    0     1    0    0    0     0      978  \n",
      "nl           0    0   996    0     0    0    0    0     0      999  \n",
      "pl           0    0     0  996     0    0    0    0     0      999  \n",
      "pt           0    0     0    0   994    0    1    0     0      996  \n",
      "ro           0    0     0    0     2  916    0    0     0      928  \n",
      "sk           1    1     0    0     2    0  890    3     0      929  \n",
      "sl           2    1     0    0     4    0    1  971     3      998  \n",
      "sv           0    0     0    0     0    0    0    0   993      996  \n",
      "__all__    990  979  1007  996  1009  917  903  974  1012    20762  \n",
      "\n",
      "[22 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = ConfusionMatrix(results['language'], results['pred_lang'])\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are generally good apart from a very poor performance for Estonian. Interestingly, Estonian is mistaken for Finnish, but not vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estonian Accuracy: 0.8410462776659959\n"
     ]
    }
   ],
   "source": [
    "et_results = results[results['language'] == 'et']\n",
    "print('Estonian Accuracy:',((et_results['language'] == et_results['pred_lang'])).sum()/len(et_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "I'm going to try training a simple LSTM model on the language data using TensorFlow to see if I can get even better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from utils import makeDF, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([chr(i) for i in range(2048)])\n",
    "encoder = LabelEncoder().fit(vocab)\n",
    "\n",
    "languages = next(os.walk('./txt'))[1]\n",
    "language_encoder = LabelEncoder().fit(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Let's process our data so it can be read by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_per_lang = 1000\n",
    "\n",
    "# Train data\n",
    "train = makeDF(os.getcwd(),sequences_per_lang)\n",
    "train['as_numbers'] = train['string'].apply(lambda x: encoder.transform(list(x)))\n",
    "train['length'] = train['string'].apply(lambda x: len(x))\n",
    "train['lang_as_numbers'] = language_encoder.transform(train['language'])\n",
    "\n",
    "# Test data\n",
    "test = pd.read_csv('europarl-test.txt', sep='\\t', header=None, names=['language', 'string'])\n",
    "test['string'] = test['string'].apply(lambda x: re.sub(r'[^\\u0000-\\u0800]', '', x))\n",
    "test = test[test['string'].apply(len) != 0] # remove empty strings\n",
    "test['as_numbers'] = test['string'].apply(lambda x: encoder.transform(list(x)))\n",
    "test['length'] = test['string'].apply(lambda x: len(x))\n",
    "test['lang_as_numbers'] = language_encoder.transform(test['language'])\n",
    "\n",
    "# Validation/test split\n",
    "test_len = len(test)\n",
    "rand_inds = np.random.choice(test_len,test_len)\n",
    "val = test.iloc[rand_inds[test_len//2:]]\n",
    "test = test.iloc[rand_inds[:test_len//2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (10414, 5)\n",
      "Validation Shape: (10414, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Test shape: ', test.shape,'\\n', 'Validation Shape: ', val.shape, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddedDataIterator():\n",
    "    def __init__(self, df, maxlen):\n",
    "        self.df = df\n",
    "        self.maxlen = maxlen\n",
    "        self.rm_too_short()\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "        \n",
    "    # Remove sequences with fewer items than maxlen\n",
    "    def rm_too_short(self):\n",
    "        self.df = self.df.drop(self.df[self.df['length'] < self.maxlen].index).reset_index(drop=True)\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.loc[self.cursor:self.cursor+n-1]\n",
    "        if len(res) != n:\n",
    "            print(res)\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        # maxlen = max(res['length'])\n",
    "        maxlen = self.maxlen\n",
    "        x = pad_sequences(res['as_numbers'].values, maxlen, padding='post', truncating='post')\n",
    "\n",
    "        return x, res['lang_as_numbers'].values, np.array([maxlen]*x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def build_graph(\n",
    "    vocab_size = len(vocab),\n",
    "    state_size = 64,\n",
    "    batch_size = 32,\n",
    "    num_classes = len(languages),\n",
    "    learning_rate = 1e-3):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, shape=[batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    y = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    keep_prob = tf.placeholder(tf.float32,[])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', shape=[vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # LSTM\n",
    "    lstmCell = tf.nn.rnn_cell.LSTMCell(state_size, name='basic_lstm_cell')\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=keep_prob)\n",
    "    rnn_outputs, _ = tf.nn.dynamic_rnn(lstmCell, rnn_inputs, dtype=tf.float32)\n",
    "    \n",
    "    # Bidirectional LSTM - doesn't really work\n",
    "    # lstmCell_fw = tf.nn.rnn_cell.LSTMCell(state_size, name='basic_lstm_cell')\n",
    "    # lstmCell_fw = tf.contrib.rnn.DropoutWrapper(cell=lstmCell_fw, output_keep_prob=keep_prob)\n",
    "    # lstmCell_bw = tf.nn.rnn_cell.LSTMCell(state_size, name='basic_lstm_cell')\n",
    "    # lstmCell_bw = tf.contrib.rnn.DropoutWrapper(cell=lstmCell_bw, output_keep_prob=keep_prob)\n",
    "    # rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell_fw, lstmCell_bw, rnn_inputs, dtype=tf.float32)\n",
    "    \n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    # rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'keep_prob': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 32, num_epochs = 14, iterator = PaddedDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        keep_prob = 0.6\n",
    "        maxlen = 64\n",
    "        tr = iterator(train, maxlen)\n",
    "        te = iterator(val, maxlen)\n",
    "        g = graph\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['keep_prob']: keep_prob}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['keep_prob']: keep_prob}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - acc:\", tr_losses[-1], \"- val_acc:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JBremner/Envs/idiom/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - acc: 0.22855227882037535 - val_acc: 0.3731094306049822\n",
      "Accuracy after epoch 2  - acc: 0.4910954301075269 - val_acc: 0.5511160714285714\n",
      "Accuracy after epoch 3  - acc: 0.650369623655914 - val_acc: 0.6696428571428571\n",
      "Accuracy after epoch 4  - acc: 0.735299059139785 - val_acc: 0.7075892857142857\n",
      "Accuracy after epoch 5  - acc: 0.7825100806451613 - val_acc: 0.7502232142857143\n",
      "Accuracy after epoch 6  - acc: 0.8182123655913979 - val_acc: 0.7854910714285714\n",
      "Accuracy after epoch 7  - acc: 0.8377016129032258 - val_acc: 0.7915178571428572\n",
      "Accuracy after epoch 8  - acc: 0.8655073924731183 - val_acc: 0.8084821428571428\n",
      "Accuracy after epoch 9  - acc: 0.889616935483871 - val_acc: 0.8379464285714285\n",
      "Accuracy after epoch 10  - acc: 0.9063340053763441 - val_acc: 0.8184151785714285\n",
      "Accuracy after epoch 11  - acc: 0.9142305107526881 - val_acc: 0.8447544642857143\n",
      "Accuracy after epoch 12  - acc: 0.912130376344086 - val_acc: 0.8128348214285714\n",
      "Accuracy after epoch 13  - acc: 0.9198588709677419 - val_acc: 0.8419642857142857\n",
      "Accuracy after epoch 14  - acc: 0.9301915322580645 - val_acc: 0.8717633928571429\n",
      "Accuracy after epoch 15  - acc: 0.9354838709677419 - val_acc: 0.8512276785714286\n",
      "Accuracy after epoch 16  - acc: 0.9354838709677419 - val_acc: 0.8665178571428571\n",
      "Accuracy after epoch 17  - acc: 0.9400201612903226 - val_acc: 0.8622767857142857\n",
      "Accuracy after epoch 18  - acc: 0.9425403225806451 - val_acc: 0.87109375\n",
      "Accuracy after epoch 19  - acc: 0.944388440860215 - val_acc: 0.8688616071428571\n",
      "Accuracy after epoch 20  - acc: 0.9487567204301075 - val_acc: 0.8700892857142857\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Absolute Discounting\n",
    "- Fix Estonia\n",
    "- LSTM accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
